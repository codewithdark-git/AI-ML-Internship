{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2551,"databundleVersionId":29345,"sourceType":"competition"}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Machine Learning libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import precision_score, recall_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\n\n# SMOTE for handling imbalanced datasets\nfrom imblearn.over_sampling import SMOTE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:55:01.145764Z","iopub.execute_input":"2025-02-09T14:55:01.146034Z","iopub.status.idle":"2025-02-09T14:55:02.686421Z","shell.execute_reply.started":"2025-02-09T14:55:01.146004Z","shell.execute_reply":"2025-02-09T14:55:02.685776Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# -------------------------------\n# Step 1: Data Loading & Preprocessing\n# -------------------------------\n\n# Load dataset (update the path to where your CSV is stored)\ndata = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-training.csv')\n\n# Check for missing values (example for MonthlyIncome and NumberOfDependents)\nprint(\"Missing values before imputation:\\n\", data[['MonthlyIncome', 'NumberOfDependents']].isnull().sum())\n\n# Impute missing values with the median\ndata['MonthlyIncome'].fillna(data['MonthlyIncome'].median(), inplace=True)\ndata['NumberOfDependents'].fillna(data['NumberOfDependents'].median(), inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:55:30.952931Z","iopub.execute_input":"2025-02-09T14:55:30.953202Z","iopub.status.idle":"2025-02-09T14:55:31.197307Z","shell.execute_reply.started":"2025-02-09T14:55:30.953181Z","shell.execute_reply":"2025-02-09T14:55:31.196202Z"}},"outputs":[{"name":"stdout","text":"Missing values before imputation:\n MonthlyIncome         29731\nNumberOfDependents     3924\ndtype: int64\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-3-ca39029ac749>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['MonthlyIncome'].fillna(data['MonthlyIncome'].median(), inplace=True)\n<ipython-input-3-ca39029ac749>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['NumberOfDependents'].fillna(data['NumberOfDependents'].median(), inplace=True)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# -------------------------------\n# Step 2: Feature Engineering\n# -------------------------------\n# Example: Create a new feature \"Income_per_Open_Credit\"\ndata['Income_per_Open_Credit'] = data['MonthlyIncome'] / (data['NumberOfOpenCreditLinesAndLoans'] + 1)\n\n# You could add other engineered features as needed\n\n# Define features and target variable\nX = data.drop(columns=['SeriousDlqin2yrs', 'Unnamed: 0'])\ny = data['SeriousDlqin2yrs']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:57:47.402065Z","iopub.execute_input":"2025-02-09T14:57:47.402403Z","iopub.status.idle":"2025-02-09T14:57:47.411683Z","shell.execute_reply.started":"2025-02-09T14:57:47.402373Z","shell.execute_reply":"2025-02-09T14:57:47.410789Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# -------------------------------\n# Step 3: Train/Test Split and SMOTE\n# -------------------------------\n# Split the data into training and testing sets with stratification\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Apply SMOTE only on the training data to balance the classes\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_res_scaled = scaler.fit_transform(X_train_res)\nX_test_scaled = scaler.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:58:10.546164Z","iopub.execute_input":"2025-02-09T14:58:10.546477Z","iopub.status.idle":"2025-02-09T14:58:10.943836Z","shell.execute_reply.started":"2025-02-09T14:58:10.546454Z","shell.execute_reply":"2025-02-09T14:58:10.943132Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# -------------------------------\n# Step 4: Model Training and Evaluation\n# -------------------------------\n# Define the models\nmodels = {\n    'RandomForest': RandomForestClassifier(random_state=42),\n    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(X_train_res_scaled, y_train_res)\n    y_pred = model.predict(X_test_scaled)\n    \n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    \n    print(f\"--- {name} ---\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(classification_report(y_test, y_pred))\n    print(\"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T14:58:16.848393Z","iopub.execute_input":"2025-02-09T14:58:16.848681Z","iopub.status.idle":"2025-02-09T14:59:37.330103Z","shell.execute_reply.started":"2025-02-09T14:58:16.848659Z","shell.execute_reply":"2025-02-09T14:59:37.329321Z"}},"outputs":[{"name":"stdout","text":"--- RandomForest ---\nPrecision: 0.3139\nRecall:    0.4244\n              precision    recall  f1-score   support\n\n           0       0.96      0.93      0.95     27995\n           1       0.31      0.42      0.36      2005\n\n    accuracy                           0.90     30000\n   macro avg       0.64      0.68      0.65     30000\nweighted avg       0.91      0.90      0.91     30000\n\n\n\n--- GradientBoosting ---\nPrecision: 0.2787\nRecall:    0.5855\n              precision    recall  f1-score   support\n\n           0       0.97      0.89      0.93     27995\n           1       0.28      0.59      0.38      2005\n\n    accuracy                           0.87     30000\n   macro avg       0.62      0.74      0.65     30000\nweighted avg       0.92      0.87      0.89     30000\n\n\n\n--- XGBoost ---\nPrecision: 0.2773\nRecall:    0.4843\n              precision    recall  f1-score   support\n\n           0       0.96      0.91      0.93     27995\n           1       0.28      0.48      0.35      2005\n\n    accuracy                           0.88     30000\n   macro avg       0.62      0.70      0.64     30000\nweighted avg       0.92      0.88      0.90     30000\n\n\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# -------------------------------\n# Step 5: Hyperparameter Tuning for All Models\n# -------------------------------\n\n# ----- Random Forest Hyperparameter Tuning -----\n\nrf_param_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2],\n}\n\nrf = RandomForestClassifier(random_state=42)\nrf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='recall', n_jobs=-1)\nrf_grid.fit(X_train_res_scaled, y_train_res)\n\nprint(\"Best parameters for Random Forest:\", rf_grid.best_params_)\nrf_best = rf_grid.best_estimator_\ny_pred_rf = rf_best.predict(X_test_scaled)\n\nprint(\"\\n--- Random Forest Performance ---\")\nprint(\"Precision:\", precision_score(y_test, y_pred_rf))\nprint(\"Recall:   \", recall_score(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Gradient Boosting Hyperparameter Tuning -----\n\ngb_param_grid = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 7],\n    'subsample': [0.8, 1.0]\n}\n\ngb = GradientBoostingClassifier(random_state=42)\ngb_grid = GridSearchCV(gb, gb_param_grid, cv=5, scoring='recall', n_jobs=-1)\ngb_grid.fit(X_train_res_scaled, y_train_res)\n\nprint(\"\\nBest parameters for Gradient Boosting:\", gb_grid.best_params_)\ngb_best = gb_grid.best_estimator_\ny_pred_gb = gb_best.predict(X_test_scaled)\n\nprint(\"\\n--- Gradient Boosting Performance ---\")\nprint(\"Precision:\", precision_score(y_test, y_pred_gb))\nprint(\"Recall:   \", recall_score(y_test, y_pred_gb))\nprint(classification_report(y_test, y_pred_gb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T15:56:06.530271Z","iopub.status.idle":"2025-02-09T15:56:06.530557Z","shell.execute_reply":"2025-02-09T15:56:06.530442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- XGBoost Hyperparameter Tuning -----\n\nxgb_param_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.8, 1.0]\n}\n\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_grid = GridSearchCV(xgb, xgb_param_grid, cv=5, scoring='recall', n_jobs=-1)\nxgb_grid.fit(X_train_res_scaled, y_train_res)\n\nprint(\"\\nBest parameters for XGBoost:\", xgb_grid.best_params_)\nxgb_best = xgb_grid.best_estimator_\ny_pred_xgb = xgb_best.predict(X_test_scaled)\n\nprint(\"\\n--- XGBoost Performance ---\")\nprint(\"Precision:\", precision_score(y_test, y_pred_xgb))\nprint(\"Recall:   \", recall_score(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T15:56:06.531416Z","iopub.status.idle":"2025-02-09T15:56:06.531792Z","shell.execute_reply":"2025-02-09T15:56:06.531630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}